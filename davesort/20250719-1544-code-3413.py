\n   from transformers import AutoModelForCausalLM, AutoTokenizer\n\n   # Load a model and tokenizer from Hugging Face\n   model_name = \"gpt2\"  # Change this to the model you'd like to use\n   model = AutoModelForCausalLM.from_pretrained(model_name)\n   tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n   # Example usage: Encode and decode text\n   inputs = tokenizer(\"Hello, Hugging Face!\", return_tensors=\"pt\")\n   outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n   decoded_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n   print(decoded_text)\n