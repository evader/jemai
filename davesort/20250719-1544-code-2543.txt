\necho \"\ud83e\udde0 Waiting for Ollama API...\"\nsleep 5\n\nif curl -s http://localhost:11434 | grep -q 'Ollama is running'; then\n  echo \"\u2705 Ollama API responding.\"\n\n  if ! docker exec ollama ollama list | grep -q \"llama3\"; then\n    echo \"\ud83d\udce5 Pulling llama3:70b model...\"\n    docker exec ollama ollama pull llama3:70b\n  fi\n\n  echo \"\ud83e\uddea Running GPT-4-style inference test...\"\n  docker exec ollama curl -s http://localhost:11434/api/generate -d '{\n    \"model\": \"llama3\",\n    \"prompt\": \"Say hello to David.\",\n    \"stream\": false\n  }' | tee /tmp/ollama_test_output.json\n\nelse\n  echo \"\u274c Ollama API not responding at localhost:11434\"\nfi\n