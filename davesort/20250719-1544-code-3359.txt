\nversion: '3.8' # Keeping version for clarity\n\nservices:\n  # JEM AI's Command & Control Backend (lt.py)\n  lt-backend:\n    build:\n      context: .\n      dockerfile: lt-backend.Dockerfile # Specify the Dockerfile name\n    container_name: jemai_lt_backend\n    ports:\n      - \"5000:5001\" # Host port 5000 -> Container port 5001 (where lt.py runs)\n    environment:\n      - GOOGLE_API_KEY=AIzaSyBUbyMDjaSy6a6WOtC2PfRH7g_YgFVTopU # Directly assigning the unpaid key\n    volumes:\n      - /etc/localtime:/etc/localtime:ro # Time synchronization\n      - /etc/timezone:/etc/timezone:ro # Timezone synchronization\n      - ./lt.py:/app/lt.py\n      - ./templates:/app/templates # Assuming templates are copied here\n    restart: unless-stopped\n\n  # Ollama server for local LLM inference\n  ollama:\n    image: ollama/ollama:latest # Pulls latest Ollama image from Docker Hub\n    container_name: jemai_ollama\n    ports:\n      - \"11434:11434\" # Ollama API port\n    volumes:\n      - ollama_data:/root/.ollama # Persistent storage for models\n      - /etc/localtime:/etc/localtime:ro # Time synchronization\n      - /etc/timezone:/etc/timezone:ro # Timezone synchronization\n      - /opt/ai_offline_setup/ai_models:/root/.ollama/models_external:ro # Mount downloaded models (adjust the path if needed)\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all # Use all detected NVIDIA GPUs\n              capabilities: [gpu] # Request GPU capabilities\n    restart: unless-stopped\n    environment:\n      - OLLAMA_LLM_MAX_MEMORY=88GB # Advertise RAM to Ollama\n      - OLLAMA_FLASH_ATTN=0 # For GPU with less than 8GB VRAM (like 1060-6GB)\n      - OLLAMA_HOST=0.0.0.0 # Ensure Ollama listens on all interfaces within Docker\n\n  # Open WebUI for Ollama Models\n  open-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    container_name: jemai_open_webui\n    ports:\n      - \"3000:8080\" # WebUI port\n    volumes:\n      - open_webui_data:/app/backend/data\n    depends_on:\n      - ollama\n    environment:\n      - OLLAMA_BASE_URL=http://jemai_ollama:11434 # Use internal service name\n    restart: unless-stopped\n\n  # Jupyter Notebooks\n  jupyter:\n    image: jupyter/tensorflow-notebook:latest\n    container_name: jemai_jupyter\n    ports:\n      - \"8888:8888\" # Jupyter port\n    volumes:\n      - /home/jemai/jemai_notebooks:/home/jovyan/work # Mount to user's home for notebooks\n      - /mnt/unraid/ai-workspace/datasets:/home/jovyan/data:ro # Mount datasets from Unraid (adjust path)\n      - /etc/localtime:/etc/localtime:ro # Time synchronization\n      - /etc/timezone:/etc/timezone:ro # Timezone synchronization\n    environment:\n      - JUPYTER_ENABLE_LAB=yes\n      - JUPYTER_TOKEN=twozee-ai-token # Example, change it as needed\n    restart: unless-stopped\n\n  # VS Code Server\n  code-server:\n    image: codercom/code-server:latest\n    container_name: jemai_code_server\n    ports:\n      - \"8080:8080\" # VS Code Server port\n    volumes:\n      - /home/jemai/jemai_projects:/home/coder/projects # Mount to user's home for projects\n      - /var/run/docker.sock:/var/run/docker.sock # Allow code-server to manage Docker\n      - /etc/localtime:/etc/localtime:ro # Time synchronization\n      - /etc/timezone:/etc/timezone:ro # Timezone synchronization\n    environment:\n      - PASSWORD=jemai-code-password # Change it as needed\n      - DOCKER_HOST=unix:///var/run/docker.sock # Instruct code-server how to talk to Docker\n    restart: unless-stopped\n\n  # Portainer (Docker Container Management UI)\n  portainer:\n    image: portainer/portainer-ce:latest\n    container_name: jemai_portainer\n    ports:\n      - \"9000:9000\" # Portainer port\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock # Access to Docker daemon\n      - portainer_data:/data # Persistent data for Portainer\n    restart: unless-stopped\n\nvolumes:\n  ollama_data: # Ollama persistent volume\n  open_webui_data: # Open WebUI persistent volume\n  portainer_data: # Portainer persistent volume\n