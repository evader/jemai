\nimport threading, queue, os, sys, subprocess, json, time\nfrom pathlib import Path\n\n# --- Placeholders for \"baked in\" modules ---\ndef ollama_chat(prompt, model=\"llama3:latest\"):\n    # Direct pipe to ollama CLI, fully local, change this per your install.\n    try:\n        result = subprocess.run(\n            [\"ollama\", \"run\", model, prompt],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            timeout=90\n        )\n        if result.returncode == 0:\n            return result.stdout.strip()\n        else:\n            return f\"[OLLAMA ERROR] {result.stderr.strip()}\"\n    except Exception as e:\n        return f\"[OLLAMA EXCEPTION] {str(e)}\"\n\ndef synapz_think(prompt):\n    # Local stub\u2014replace with real RAG/agent call as needed.\n    return f\"[SYNAPZ] Thinking about: {prompt}\"\n\ndef read_file(path):\n    try:\n        with open(path, \"r\") as f:\n            return f.read()\n    except Exception as e:\n        return f\"[READ ERROR] {e}\"\n\ndef write_file(path, content):\n    try:\n        with open(path, \"w\") as f:\n            f.write(content)\n        return \"[WRITE] Success.\"\n    except Exception as e:\n        return f\"[WRITE ERROR] {e}\"\n\ndef run_shell(cmd):\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)\n        return result.stdout.strip() if result.returncode == 0 else result.stderr.strip()\n    except Exception as e:\n        return f\"[SHELL ERROR] {e}\"\n\ndef list_dir(path):\n    try:\n        return \"\\n\".join(os.listdir(path))\n    except Exception as e:\n        return f\"[LS ERROR] {e}\"\n\nclass JemaiCore:\n    def __init__(self):\n        self.running = True\n        self.context = []\n        self.current_model = \"llama3:latest\"\n        self.rag_enabled = True\n\n    def input_loop(self):\n        print(f\"=== JEMAI CLI (AI+Shell+Synapz+RAG) ===\")\n        print(\"Type normal text for AI. Shell: !cmd. Read: read <file>. Write: write <file>. RAG: rag <query>. Quit: exit\\n\")\n        while self.running:\n            try:\n                user = input(\"> \").strip()\n                if not user: continue\n                if user in [\"exit\", \"quit\"]:\n                    self.running = False\n                    print(\"Goodbye.\")\n                    break\n                # --- Command parse ---\n                if user.startswith(\"!\"):\n                    print(run_shell(user[1:]))\n                elif user.startswith(\"read \"):\n                    print(read_file(user[5:].strip()))\n                elif user.startswith(\"write \"):\n                    path = user[6:].strip()\n                    print(f\"(Paste new content for {path}, Ctrl-D to save)\")\n                    lines = []\n                    while True:\n                        try: lines.append(input())\n                        except EOFError: break\n                    print(write_file(path, \"\\n\".join(lines)))\n                elif user.startswith(\"ls \"):\n                    print(list_dir(user[3:].strip()))\n                elif user.startswith(\"rag \"):\n                    print(synapz_think(user[4:].strip()))\n                elif user.startswith(\"model \"):\n                    self.current_model = user[6:].strip()\n                    print(f\"[MODEL] Now chatting with: {self.current_model}\")\n                else:\n                    # Default: AI chat (Ollama model)\n                    ai_reply = ollama_chat(user, self.current_model)\n                    print(ai_reply)\n                    self.context.append({\"user\": user, \"ai\": ai_reply})\n            except (KeyboardInterrupt, EOFError):\n                self.running = False\n                print(\"\\nGoodbye.\")\n\nif __name__ == \"__main__\":\n    JemaiCore().input_loop()\n