# ===================================================================
#  JEM AI - Production Docker Compose (V2.1 - Unified Core Full Stack)
# ===================================================================
#  This file orchestrates the entire JEM AI ecosystem.
#  It uses variables from the adjacent '.env' file for configuration.
# ===================================================================

services:
  # The new, unified JEM AI Brain and Web Server
  jemai_core:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: jemai_prod_core
    ports:
      - "5000:5000"    # JEM AI Shell UI
      - "5678:5678"    # VSCode Debugger Port
    volumes:
      - .:/app         # Mounts the entire docker_setup dir for LIVE EDITING
      - "/var/run/docker.sock:/var/run/docker.sock" # Allows control over Docker
      - "${PROD_RAG_DATA_DIR}:/app/rag/chroma_data" # Mounts the RAG memory
      - ollama_prod_data:/root/.ollama             # Persistent storage for Ollama models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST}
      - TZ=Australia/Perth
    # The command is overridden by the devcontainer config for debugging
    command: python3 -u jemai_core.py

  # Open WebUI - The 'sexy' frontend for direct Ollama interaction
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: jemai_prod_open_webui
    ports:
      - "3000:8080"
    depends_on:
      - jemai_core # It needs the Ollama library inside jemai_core now
    environment:
      # Open WebUI talks to the Ollama API, which is now part of jemai_core, exposed on the Docker network
      # However, Ollama's API itself is not exposed by jemai_core's Flask app.
      # This needs a slight re-architecture. The simplest immediate fix is to have
      # Open WebUI talk to a standalone Ollama service.
      # For now, this is a known issue we will fix. Let's stand up the standalone Ollama again.
      - OLLAMA_BASE_URL=http://jemai_prod_ollama:11434
    restart: unless-stopped

  # Standalone Ollama Service (Restored for compatibility with Open WebUI)
  ollama:
    image: ollama/ollama:latest
    container_name: jemai_prod_ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_prod_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST}
      - TZ=Australia/Perth
      
  # JupyterLab - Your AI & Data Science Workbench
  jupyter:
    image: jupyter/tensorflow-notebook:latest
    container_name: jemai_prod_jupyter
    ports:
      - "8888:8888"
    volumes:
      - "${USER_NOTEBOOKS_DIR}:/home/jovyan/work"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=${JUPYTER_TOKEN}
      - TZ=Australia/Perth
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Code-Server - Your "God Mode" IDE
  code-server:
    image: codercom/code-server:latest
    container_name: jemai_prod_code_server
    ports:
      - "8080:8080"
    volumes:
      - "/:/host"  # "God Mode" mount for full filesystem access
      - "/var/run/docker.sock:/var/run/docker.sock"
    environment:
      - PASSWORD=${CODE_SERVER_PASSWORD}
      - DOCKER_HOST=unix:///var/run/docker.sock
      - TZ=Australia/Perth
    restart: unless-stopped

volumes:
  ollama_prod_data:
  # Portainer has been removed as per your directive for simplification.