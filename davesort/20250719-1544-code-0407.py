\n#!/usr/bin/env python3\nimport os, sys, threading, time, queue, shutil, subprocess, platform, tempfile, json\nfrom pathlib import Path\n\n# ==== CORE CONFIGS ====\nJEMAI_ROOT = Path(__file__).parent.resolve()\nDATA_ROOT = JEMAI_ROOT / \"jemai_data\"\nDATA_ROOT.mkdir(exist_ok=True)\nMODELS_ROOT = Path(\"/usr/share/ollama/.ollama/models/blobs\")  # Change if your models elsewhere\nSYNAPZ_CORE = DATA_ROOT / \"synapz_core.py\"\nVOICE_FLAG = DATA_ROOT / \"voice_enabled\"\nHOME_ASSISTANT_FLAG = DATA_ROOT / \"ha_enabled\"\n\n# ==== SYNAPZ CORE - Default \"starter\" code ====\nDEFAULT_SYN_CORE = '''\nimport os, subprocess\ndef synapz_command(cmd):\n    # Simple example: run any shell command (patch for more AI power!)\n    try:\n        out = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT, timeout=10)\n        return out.decode().strip()\n    except Exception as e:\n        return f\"[SYNAPZ ERROR] {e}\"\n'''\n\nif not SYNAPZ_CORE.exists():\n    SYNAPZ_CORE.write_text(DEFAULT_SYN_CORE)\n\n# ==== AGENT: HOT PATCHING SUPPORT ====\ndef upgrade_agent():\n    print(\"Paste your Python code to patch/replace Synapz core (Ctrl-D to finish):\")\n    code = \"\"\n    while True:\n        try: line = input()\n        except EOFError: break\n        code += line + \"\\n\"\n    SYNAPZ_CORE.write_text(code)\n    print(\"[UPGRADE] Synapz core replaced and hot-reloaded.\")\n\ndef load_synapz():\n    import importlib.util\n    spec = importlib.util.spec_from_file_location(\"synapz_core\", SYNAPZ_CORE)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    return module\n\n# ==== AGENT: OLLAMA + RAG MODEL INTEGRATION ====\ndef refresh_ollama_models():\n    print(\"Rescanning Ollama models (local):\")\n    if MODELS_ROOT.exists():\n        found = False\n        for f in MODELS_ROOT.glob(\"*\"):\n            print(f\"- {f.name}\")\n            found = True\n        if not found:\n            print(\"[ERR] No models found in blobs dir.\")\n    else:\n        print(\"[ERR] Ollama model dir missing.\")\n\ndef ollama_query(model, prompt):\n    # Local HTTP query to Ollama (no docker needed!)\n    import requests\n    try:\n        res = requests.post(\n            \"http://localhost:11434/api/generate\",\n            json={\"model\": model, \"prompt\": prompt, \"stream\": False},\n            timeout=60,\n        )\n        if res.ok and \"response\" in res.json():\n            return res.json()[\"response\"]\n        else:\n            return \"[OLLAMA ERROR] \" + res.text\n    except Exception as e:\n        return f\"[OLLAMA ERROR] {e}\"\n\n# ==== AGENT: VOICE SYNTH (cross-platform) ====\ndef speak(text):\n    try:\n        import pyttsx3\n        engine = pyttsx3.init()\n        engine.say(text)\n        engine.runAndWait()\n    except Exception as e:\n        print(f\"[VOICE ERR] {e}\")\n\n# ==== AGENT: HOME ASSISTANT (stub, can patch later) ====\ndef ha_action(cmd):\n    print(f\"[HA] Would run HA action: {cmd} (stub, patch to enable!)\")\n\n# ==== AGENT: AUTO BUILDER ====\ndef auto_builder():\n    print(\"[BUILDER] Self-repair/rebuild started (stub).\")\n    # Merge modules, self-test, patch whatever's uploaded/pasted next...\n\n# ==== AGENT: ANY KEY KEEPS BUILDING ====\ndef keep_building():\n    print(\"[JEMAI] No input. System ready for next build or patch. (Paste code, commands, or just ask!)\")\n\n# ==== MAIN CLI LOOP ====\ndef main():\n    print(f\"\"\"\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\n\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\n\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\nJEMAI OS / SYNAPZ AGENT \u2014 [RUNNING]\n    \"\"\")\n    print(\"Type a shell command, ask for anything, or paste code to patch the AI. Type 'exit' to quit.\")\n    # Load Synapz (hot patchable)\n    synapz = load_synapz()\n    ollama_model = \"llama3:latest\"  # Default, but list available models\n\n    while True:\n        try:\n            cmd = input(\"\\nJEMAI> \").strip()\n        except (EOFError, KeyboardInterrupt):\n            print(\"Goodbye!\")\n            break\n\n        if not cmd:\n            keep_building()\n            continue\n\n        if cmd.lower() in (\"exit\", \"quit\", \"bye\"):\n            print(\"Shutting down.\")\n            break\n\n        if cmd.lower() in (\"upgrade jemai\", \"patch synapz\", \"paste code\"):\n            upgrade_agent()\n            synapz = load_synapz()\n            continue\n\n        if \"ollama\" in cmd.lower() and \"refresh\" in cmd.lower():\n            refresh_ollama_models()\n            continue\n\n        if cmd.lower() in (\"build all\", \"warmwinds build\"):\n            auto_builder()\n            continue\n\n        if cmd.lower().startswith(\"speak \"):\n            text = cmd[6:].strip()\n            speak(text)\n            continue\n\n        if cmd.lower().startswith(\"ha \"):\n            ha_action(cmd[3:].strip())\n            continue\n\n        # RAG/Chroma/History placeholder (add merge/paste logic here)\n        if cmd.lower().startswith(\"merge rag\"):\n            print(\"[RAG] Paste your RAG data (Ctrl-D to finish):\")\n            rag_data = \"\"\n            while True:\n                try: line = input()\n                except EOFError: break\n                rag_data += line + \"\\n\"\n            (DATA_ROOT / \"rag_history.txt\").write_text(rag_data)\n            print(\"[RAG] Data saved.\")\n            continue\n\n        # Model swapper (eg. \"model: llama2:latest\")\n        if cmd.lower().startswith(\"model:\"):\n            new_model = cmd.split(\":\",1)[1].strip()\n            print(f\"[OLLAMA] Switching to model: {new_model}\")\n            ollama_model = new_model\n            continue\n\n        # ---- COMMAND/ACTION ROUTER ----\n        # Try to parse as shell, then as AI\n        if cmd.split()[0] in (\"ls\", \"cat\", \"cd\", \"cp\", \"mv\", \"rm\", \"ps\", \"grep\", \"find\", \"chmod\", \"chown\", \"touch\"):\n            try:\n                out = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT, timeout=10)\n                print(out.decode())\n            except Exception as e:\n                print(f\"[SHELL ERR] {e}\")\n            continue\n\n        # Otherwise: Try Synapz \"AI shell\", then Ollama, then fallback\n        try:\n            # Try Synapz AI (can be upgraded to full RAG+voice+HA)\n            if hasattr(synapz, \"synapz_command\"):\n                response = synapz.synapz_command(cmd)\n                print(f\"[SYNAPZ]: {response}\")\n                if VOICE_FLAG.exists(): speak(response)\n                continue\n        except Exception as e:\n            print(f\"[SYNAPZ ERROR] {e}\")\n\n        # If no Synapz, fall back to Ollama local\n        print(f\"[OLLAMA: {ollama_model}] > {cmd}\")\n        out = ollama_query(ollama_model, cmd)\n        print(out)\n        if VOICE_FLAG.exists():\n            speak(out)\n\n# ==== ENTRY POINT ====\nif __name__ == \"__main__\":\n    main()\n