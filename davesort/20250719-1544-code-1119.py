\n# synapz_rag_ingest.py\n# Ingests Gemini/Synapz chat history and loads it into ChromaDB\n\nimport json\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\nimport os\n\n# --- CONFIG ---\nCHROMA_DIR = \"./chroma_data\"\nEMBED_MODEL = \"BAAI/bge-small-en-v1.5\"  # You can replace with any local embedding model\nCHUNK_SIZE = 500\nCHUNK_OVERLAP = 100\nINPUT_FILE = \"SynapzV2_conversation_history.json\"\n\n# --- Load Chat History ---\nwith open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n    raw = json.load(f)\n\nmessages = raw.get(\"messages\", [])\n\n# --- Convert to Documents ---\ndocs = []\nfor i, msg in enumerate(messages):\n    role = msg[\"author\"]\n    parts = msg[\"content\"].get(\"parts\", [])\n    text = \"\\n\".join(p[\"text\"] for p in parts if p.get(\"text\"))\n    if not text.strip():\n        continue\n    metadata = {\"role\": role, \"message_index\": i}\n    docs.append(Document(page_content=text, metadata=metadata))\n\n# --- Chunk Documents ---\nsplitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\nchunks = splitter.split_documents(docs)\n\n# --- Setup Embeddings ---\nembedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n\n# --- Create ChromaDB Vector Store ---\nif not os.path.exists(CHROMA_DIR):\n    os.makedirs(CHROMA_DIR)\n\nvectorstore = Chroma.from_documents(documents=chunks, embedding=embedder, persist_directory=CHROMA_DIR)\nvectorstore.persist()\n\nprint(f\"\u2705 {len(chunks)} chunks embedded and stored to {CHROMA_DIR}\")\n