\n# synapz_rag_query.py\n# Local CLI script to query embedded Gemini history using ChromaDB + Ollama\n\nimport os\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.llms import Ollama\nfrom langchain.chains import RetrievalQA\n\n# --- CONFIG ---\nCHROMA_DIR = \"./chroma_data\"\nEMBED_MODEL = \"BAAI/bge-small-en-v1.5\"\nOLLAMA_MODEL = \"llama3:8b\"  # Change this to whatever you've pulled\n\n# --- Load Embeddings & Vectorstore ---\nembedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\ndb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embedder)\n\n# --- Set up Retriever + LLM ---\nretriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\nllm = Ollama(model=OLLAMA_MODEL)\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=retriever,\n    return_source_documents=True\n)\n\n# --- CLI Loop ---\nprint(\"\\n\ud83d\udd0d Synapz Local RAG Ready. Ask anything about your history.\")\nwhile True:\n    query = input(\"\\n\ud83e\udde0 You: \")\n    if query.strip().lower() in [\"exit\", \"quit\"]:\n        break\n    response = qa_chain(query)\n    print(f\"\\n\ud83e\udd16 Synapz: {response['result']}\")\n