\nimport os\nimport re\nimport json\nimport csv\nimport hashlib\nimport sqlite3\nfrom pathlib import Path\nimport importlib\n\nDATA_DIR = r\"C:\\JEMAI_HUB\"\nPLUGINS_DIR = Path(DATA_DIR) / \"plugins\"\nCHUNK_SIZE = 1000\nEXPORT_JSONL = True\nEXPORT_CSV = True\n\nos.makedirs(DATA_DIR, exist_ok=True)\nos.makedirs(PLUGINS_DIR, exist_ok=True)\n\ndef hash_text(text):\n    clean = re.sub(r'\\s+', ' ', text.strip().lower())\n    return hashlib.sha256(clean.encode(\"utf-8\")).hexdigest()\n\ndef yield_chunks(text, chunk_size=CHUNK_SIZE):\n    words = text.split()\n    for i in range(0, len(words), chunk_size):\n        yield \" \".join(words[i:i+chunk_size])\n\ndef fallback_parser(filepath):\n    try:\n        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            text = f.read()\n        return [{\n            \"source\": \"unknown\",\n            \"title\": os.path.basename(filepath),\n            \"text\": text,\n            \"date\": \"\",\n            \"metadata\": {\"origin\": str(filepath)}\n        }]\n    except Exception:\n        return []\n\ndef discover_plugins():\n    parsers = []\n    for fname in os.listdir(PLUGINS_DIR):\n        if fname.endswith(\".py\") and not fname.startswith(\"__\"):\n            mod = importlib.import_module(f\"plugins.{fname[:-3]}\")\n            if hasattr(mod, \"register\"):\n                mod.register(parsers.append)\n    return parsers\n\ndef load_existing_hashes_sqlite(sqlite_path):\n    hashes = set()\n    if not os.path.exists(sqlite_path):\n        return hashes\n    conn = sqlite3.connect(sqlite_path)\n    c = conn.cursor()\n    c.execute(\"CREATE TABLE IF NOT EXISTS chunks (hash TEXT PRIMARY KEY, source TEXT, title TEXT, text TEXT, date TEXT, meta TEXT)\")\n    for row in c.execute(\"SELECT hash FROM chunks\"):\n        hashes.add(row[0])\n    conn.close()\n    return hashes\n\ndef add_to_sqlite(sqlite_path, chunk):\n    conn = sqlite3.connect(sqlite_path)\n    c = conn.cursor()\n    c.execute(\"CREATE TABLE IF NOT EXISTS chunks (hash TEXT PRIMARY KEY, source TEXT, title TEXT, text TEXT, date TEXT, meta TEXT)\")\n    c.execute(\"INSERT OR IGNORE INTO chunks VALUES (?, ?, ?, ?, ?, ?)\", (\n        chunk['hash'], chunk['source'], chunk['title'], chunk['text'], chunk.get('date', ''), json.dumps(chunk.get('metadata', {}))\n    ))\n    conn.commit()\n    conn.close()\n\ndef write_jsonl(jsonl_path, chunk):\n    with open(jsonl_path, \"a\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n\ndef write_csv(csv_path, chunk, fieldnames=None):\n    exists = os.path.exists(csv_path)\n    with open(csv_path, \"a\", encoding=\"utf-8\", newline='') as f:\n        writer = csv.DictWriter(f, fieldnames=fieldnames or [\"hash\", \"source\", \"title\", \"text\", \"date\"])\n        if not exists:\n            writer.writeheader()\n        writer.writerow({k: chunk.get(k, \"\") for k in writer.fieldnames})\n\ndef main():\n    # Discover plugins\n    import sys\n    sys.path.insert(0, str(Path(DATA_DIR)))  # For plugins import\n    parsers = discover_plugins()\n    print(f\"Loaded {len(parsers)} parser plugins.\")\n\n    # Find files to process\n    files_to_scan = []\n    for root, dirs, files in os.walk(DATA_DIR):\n        # Skip the output DB/JSONL/CSV and plugins themselves\n        if root.startswith(str(PLUGINS_DIR)) or root == DATA_DIR:\n            continue\n        for f in files:\n            path = Path(root) / f\n            if \"jemai_hub\" in path.name:  # skip outputs\n                continue\n            files_to_scan.append(str(path))\n\n    # Prepare output\n    sqlite_path = os.path.join(DATA_DIR, \"jemai_hub.sqlite3\")\n    jsonl_path = os.path.join(DATA_DIR, \"jemai_hub.jsonl\")\n    csv_path = os.path.join(DATA_DIR, \"jemai_hub.csv\")\n    existing_hashes = load_existing_hashes_sqlite(sqlite_path)\n    seen = set(existing_hashes)\n    n_total = n_unique = n_dupe = 0\n\n    for path in files_to_scan:\n        handled = False\n        for parser in parsers:\n            try:\n                results = parser(path)\n                if results:\n                    handled = True\n                    break\n            except Exception as e:\n                continue\n        if not handled:\n            results = fallback_parser(path)\n        for conv in results:\n            for chunk_text in yield_chunks(conv[\"text\"], CHUNK_SIZE):\n                chunk_hash = hash_text(chunk_text)\n                n_total += 1\n                if chunk_hash in seen:\n                    n_dupe += 1\n                    continue\n                seen.add(chunk_hash)\n                chunk = {\n                    \"hash\": chunk_hash,\n                    \"source\": conv.get(\"source\", \"unknown\"),\n                    \"title\": conv.get(\"title\", \"\"),\n                    \"text\": chunk_text,\n                    \"date\": conv.get(\"date\", \"\"),\n                    \"metadata\": conv.get(\"metadata\", {})\n                }\n                add_to_sqlite(sqlite_path, chunk)\n                if EXPORT_JSONL:\n                    write_jsonl(jsonl_path, chunk)\n                if EXPORT_CSV:\n                    write_csv(csv_path, chunk)\n                n_unique += 1\n\n    print(f\"\\nALL DONE! {n_total} total chunks scanned, {n_unique} unique chunks imported, {n_dupe} exact duplicates skipped.\")\n    print(f\"Outputs:\\n  {sqlite_path}\\n  {jsonl_path}\\n  {csv_path}\\n\")\n    print(\"Add new plugins to /plugins, rerun script any time!\")\n\nif __name__ == \"__main__\":\n    main()\n