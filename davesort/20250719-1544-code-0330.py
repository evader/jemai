\n# synapz_main.py\n\nimport os, sys, threading, queue\nimport synapz_core\nfrom llama_cpp import Llama\nimport chromadb\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\nimport pty, subprocess\n\nMODEL_PATH = \"/usr/share/ollama/.ollama/models/blobs/llama3-*.gguf\"  # glob as before\n# ...model glob logic...\n\nllm = Llama(model_path=MODEL_PATH, n_ctx=4096, n_gpu_layers=0)\nembedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nvectordb = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding)\nvectordb.persist()\n\n# Real PTY shell (shared, persistent)\ndef shell_session(io_queue):\n    pid, fd = pty.fork()\n    if pid == 0:\n        os.execvp(\"bash\", [\"bash\"])\n    else:\n        while True:\n            data = os.read(fd, 1024)\n            if data:\n                io_queue.put(data.decode())\n\nshell_q = queue.Queue()\nthreading.Thread(target=shell_session, args=(shell_q,), daemon=True).start()\n\ndef send_shell(cmd):\n    # write to PTY\n    pass  # TODO: send input to bash\n\ndef process_msg(msg):\n    # Synapz intent routing\n    if \"ls\" in msg or \"cat \" in msg or \"rm \" in msg or \"cp \" in msg:\n        send_shell(msg)\n        return shell_q.get(timeout=2)\n    if \"rag\" in msg:\n        return str(vectordb.similarity_search(msg, k=3))\n    # else fallback to Synapz+LLM blend\n    enriched = synapz_core.enrich_prompt(msg)\n    return llm(enriched)['choices'][0]['text']\n\n# Mini-API for Electron UI (via stdin/stdout, no HTTP at all)\ndef main_loop():\n    while True:\n        msg = sys.stdin.readline()\n        if not msg:\n            break\n        print(process_msg(msg.strip()), flush=True)\n\nif __name__ == \"__main__\":\n    main_loop()\n