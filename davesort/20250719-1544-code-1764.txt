\n#!/bin/bash\nset -e\n\necho \"\ud83e\uddf9 Cleaning up old Docker containers, images, volumes...\"\ndocker compose down --volumes --remove-orphans || true\ndocker system prune --all --volumes --force\n\necho \"\ud83d\udcc1 Writing Dockerfile...\"\ncat > Dockerfile <<'EOF'\nFROM python:3.10-slim-bookworm\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY . .\n\nEXPOSE 5000 5678\n\nCMD [\"python3\", \"-m\", \"debugpy\", \"--listen\", \"0.0.0.0:5678\", \"--wait-for-client\", \"-m\", \"flask\", \"run\", \"--app\", \"jemai_core.py\", \"--host=0.0.0.0\", \"--port=5000\"]\nEOF\n\necho \"\ud83d\udcc1 Writing requirements.txt...\"\ncat > requirements.txt <<'EOF'\nFlask==3.0.3\nFlask-SocketIO==5.3.6\neventlet==0.35.2\npsutil==5.9.8\ndocker==7.1.0\nlangchain==0.2.1\nlangchain-community==0.2.1\nchromadb==0.5.0\nsentence-transformers==2.7.0\nollama==0.2.1\npytz==2024.1\ndebugpy==1.8.1\nEOF\n\necho \"\ud83d\udcc1 Writing docker-compose.yml...\"\ncat > docker-compose.yml <<'EOF'\nversion: '3.9'\n\nservices:\n  jemai_core:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    container_name: jemai_prod_core\n    ports:\n      - \"5000:5000\"\n      - \"5678:5678\"\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    volumes:\n      - \".:/app\"\n      - \"/var/run/docker.sock:/var/run/docker.sock\"\n      - \"${PROD_RAG_DATA_DIR}:/app/rag/chroma_data\"\n    depends_on:\n      - ollama\n    restart: unless-stopped\n    environment:\n      - FLASK_APP=jemai_core.py\n      - OLLAMA_HOST=http://jemai_prod_ollama:11434\n      - TZ=Australia/Perth\n\n  ollama:\n    image: ollama/ollama:latest\n    container_name: jemai_prod_ollama\n    ports:\n      - \"11434:11434\"\n    volumes:\n      - ollama_prod_data:/root/.ollama\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n    restart: unless-stopped\n    environment:\n      - OLLAMA_HOST=0.0.0.0\n      - TZ=Australia/Perth\n\nvolumes:\n  ollama_prod_data:\nEOF\n\necho \"\ud83d\udcc1 Writing jemai_core.py (Flask app with RAG + Ollama integration)...\"\ncat > jemai_core.py <<'EOF'\nimport os\nimport sys\nimport threading\nimport json\nimport requests\nimport logging\nimport time\nfrom datetime import datetime\nimport docker\nfrom flask import Flask, request, jsonify, send_from_directory\nfrom flask_socketio import SocketIO\nfrom pytz import timezone\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.llms import Ollama\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n\nPERTH_TZ = timezone('Australia/Perth')\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - Synapz - %(levelname)s - %(message)s',\n                    datefmt='%d/%m/%y %H:%M:%S')\nlogging.Formatter.converter = lambda *args: datetime.now(PERTH_TZ).timetuple()\nlogger = logging.getLogger(__name__)\n\napp = Flask(__name__, static_folder='static', template_folder='templates')\nsocketio = SocketIO(app)\n\nqa_chain = None\nrag_lock = threading.Lock()\n\ndef initialize_rag():\n    global qa_chain\n    with rag_lock:\n        if qa_chain is not None and qa_chain != \"INIT_FAILED\":\n            logger.info(\"RAG already initialized.\")\n            return\n\n        logger.info(\"\ud83e\udde0 Initializing Synapz RAG Brain...\")\n        CHROMA_DIR = \"/app/rag/chroma_data\"\n\n        if not os.path.exists(CHROMA_DIR):\n            logger.error(f\"RAG DB not found at {CHROMA_DIR}\")\n            qa_chain = \"INIT_FAILED\"\n            return\n\n        if not os.listdir(CHROMA_DIR):\n            logger.warning(f\"RAG DB directory {CHROMA_DIR} is empty.\")\n\n        try:\n            ollama_url = os.environ.get(\"OLLAMA_HOST\", \"http://jemai_prod_ollama:11434\")\n            logger.info(f\"Connecting to Ollama at {ollama_url}...\")\n            requests.head(ollama_url, timeout=10)\n            logger.info(\"Ollama detected, proceeding.\")\n\n            embedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", model_kwargs={'device': 'cuda'})\n            db = Chroma(persist_directory=CHROMA_DIR, embedding_function=embedder)\n            retriever = db.as_retriever(search_kwargs={\"k\": 5})\n            llm = Ollama(base_url=ollama_url, model=\"mistral:7b\")\n\n            template = \"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer: \"\n            QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n\n            qa_chain = RetrievalQA.from_chain_type(\n                llm=llm,\n                chain_type=\"stuff\",\n                retriever=retriever,\n                chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n                return_source_documents=False\n            )\n            logger.info(\"\u2705 Synapz RAG Brain online.\")\n        except requests.exceptions.ConnectionError:\n            logger.error(f\"\u274c Ollama not reachable at {ollama_url}\")\n            qa_chain = \"INIT_FAILED\"\n        except Exception as e:\n            logger.error(f\"\u274c RAG init failed: {e}\", exc_info=True)\n            qa_chain = \"INIT_FAILED\"\n\n@app.route('/')\ndef index():\n    return send_from_directory(app.template_folder, 'index.html')\n\n@socketio.on('query_rag')\ndef handle_rag_query(data):\n    sid = request.sid\n    query = data.get('query')\n    logger.info(f\"RAG query from {sid}: '{query}'\")\n\n    if qa_chain is None:\n        socketio.emit('rag_response', {\"response\": \"Waking up brain, please wait...\"}, room=sid)\n        socketio.start_background_task(initialize_rag)\n\n    if qa_chain == \"INIT_FAILED\":\n        socketio.emit('rag_response', {\"error\": \"RAG not initialized. Check logs.\"}, room=sid)\n        return\n\n    def run_query():\n        retries = 0\n        MAX_WAIT = 60\n        while qa_chain is None and retries < MAX_WAIT:\n            logger.info(f\"Waiting RAG init ({retries+1}/{MAX_WAIT}s)...\")\n            time.sleep(1)\n            retries += 1\n        if qa_chain == \"INIT_FAILED\":\n            socketio.emit('rag_response', {\"error\": \"RAG init failed after waiting.\"}, room=sid)\n            return\n        elif qa_chain is None:\n            socketio.emit('rag_response', {\"error\": \"RAG init timed out.\"}, room=sid)\n            return\n\n        try:\n            result = qa_chain.run(query)\n            logger.info(f\"RAG processed response length: {len(result)}\")\n            socketio.emit('rag_response', {\"response\": result}, room=sid)\n        except Exception as e:\n            logger.error(f\"RAG query error: {e}\", exc_info=True)\n            err_str = str(e)\n            if \"ConnectionError\" in err_str:\n                socketio.emit('rag_response', {\"error\": \"Ollama unreachable during query.\"}, room=sid)\n            elif \"CUDA out of memory\" in err_str:\n                socketio.emit('rag_response', {\"error\": \"GPU out of memory.\"}, room=sid)\n            else:\n                socketio.emit('rag_response', {\"error\": f\"Query error: {err_str}\"}, room=sid)\n\n    socketio.start_background_task(run_query)\n\nif __name__ == '__main__':\n    logger.info(\"--- Starting JEM AI C&C Web Server ---\")\n    debug_mode = os.environ.get('DEBUG_MODE', 'False').lower() == 'true'\n    socketio.run(app, host='0.0.0.0', port=5000, debug=debug_mode, allow_unsafe_werkzeug=True)\nEOF\n\necho \"\ud83d\ude80 Building Docker image and starting containers...\"\ndocker compose build --no-cache\ndocker compose up -d\n\necho \"\u2705 Setup complete! Use the following commands to monitor and interact:\"\necho \"   docker compose logs jemai_core --tail=100 -f\"\necho \"   docker exec -it jemai_prod_core bash\"\n\necho \"\ud83d\udd27 Done.\"\n