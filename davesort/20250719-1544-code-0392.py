\nimport threading, queue, os, sys, subprocess, json, time\nfrom pathlib import Path\n\nclass JemaiCore:\n    def __init__(self):\n        self.running = True\n        self.context = []\n        self.current_model = \"llama3:latest\"\n        self.jobs = queue.Queue()\n        self.rag_enabled = True\n        self.autosave_path = \"jemai_ctx.json\"\n        self.node_bus = {}\n        self._start_modules()\n\n    def _start_modules(self):\n        threading.Thread(target=self.hotword_daemon, daemon=True).start()\n        threading.Thread(target=self.voice_daemon, daemon=True).start()\n        threading.Thread(target=self.ha_daemon, daemon=True).start()\n        threading.Thread(target=self.debugger_daemon, daemon=True).start()\n        threading.Thread(target=self.anykey_builder, daemon=True).start()\n\n    def input_loop(self):\n        print(f\"=== JEMAI Warmwinds v3: Voice, Home Assistant, Modular Node Bus ===\")\n        self.load_ctx()\n        while self.running:\n            try:\n                user = input(\"> \").strip()\n                if not user: continue\n                if user.lower() in [\"exit\", \"quit\"]:\n                    self.running = False\n                    print(\"Goodbye.\")\n                    break\n                # Node ops\n                if user.startswith(\"add node\"):\n                    node = user.split(maxsplit=2)[-1]\n                    self.add_node(node)\n                    continue\n                # Home Assistant quickies\n                if user.startswith(\"ha \"):\n                    print(self.ha_command(user[3:]))\n                    continue\n                # Speak out loud!\n                if user.startswith(\"say \"):\n                    self.say(user[4:])\n                    continue\n                # All-in chat/shell/ops\n                if user.startswith(\"!\"):\n                    print(run_shell(user[1:]))\n                elif user.startswith(\"read \"):\n                    print(read_file(user[5:].strip()))\n                elif user.startswith(\"write \"):\n                    path = user[6:].strip()\n                    print(f\"(Paste new content for {path}, Ctrl-D to save)\")\n                    lines = []\n                    while True:\n                        try: lines.append(input())\n                        except EOFError: break\n                    print(write_file(path, \"\\n\".join(lines)))\n                elif user.startswith(\"ls \"):\n                    print(list_dir(user[3:].strip()))\n                elif user.startswith(\"rag \"):\n                    print(synapz_think(user[4:].strip()))\n                elif user.startswith(\"model \"):\n                    self.current_model = user[6:].strip()\n                    print(f\"[MODEL] Now chatting with: {self.current_model}\")\n                elif user.startswith(\"save\"):\n                    self.save_ctx()\n                    print(\"[CTX] Context saved.\")\n                elif user.startswith(\"load\"):\n                    self.load_ctx()\n                    print(\"[CTX] Context loaded.\")\n                else:\n                    # Default: AI chat\n                    ai_reply = ollama_chat(user, self.current_model)\n                    print(ai_reply)\n                    self.say(ai_reply) # <-- Actually speaks the AI reply\n                    self.context.append({\"user\": user, \"ai\": ai_reply})\n                    self.save_ctx()\n            except (KeyboardInterrupt, EOFError):\n                self.running = False\n                print(\"\\nGoodbye.\")\n                self.save_ctx()\n\n    def say(self, text):\n        \"\"\"Speak aloud using system TTS (Linux: espeak, Mac: say, Win: powershell)\"\"\"\n        print(\"[JEMAI VOICE] \" + text)\n        try:\n            if sys.platform.startswith(\"darwin\"):\n                subprocess.Popen([\"say\", text])\n            elif sys.platform.startswith(\"linux\"):\n                subprocess.Popen([\"espeak\", text])\n            elif sys.platform.startswith(\"win\"):\n                subprocess.Popen([\"powershell\", \"-c\", f\"Add-Type \u2013AssemblyName System.Speech; (New-Object System.Speech.Synthesis.SpeechSynthesizer).Speak('{text}')\"])\n        except Exception as e:\n            print(f\"[TTS ERROR] {e}\")\n\n    def add_node(self, node_name):\n        # TODO: Real node integration (network scan, MQTT, ssh, etc)\n        self.node_bus[node_name] = {\"status\": \"attached\", \"type\": \"stub\"}\n        print(f\"[NODE BUS] Added node: {node_name} (stub)\")\n\n    def ha_command(self, cmd):\n        # TODO: Replace with real Home Assistant API calls!\n        return f\"[HA STUB] Would do: {cmd}\"\n\n    # ---- File/context ops ----\n    def save_ctx(self):\n        try:\n            with open(self.autosave_path, \"w\") as f:\n                json.dump(self.context, f)\n        except Exception as e:\n            print(f\"[SAVE ERROR] {e}\")\n\n    def load_ctx(self):\n        if os.path.exists(self.autosave_path):\n            try:\n                with open(self.autosave_path) as f:\n                    self.context = json.load(f)\n            except Exception as e:\n                print(f\"[LOAD ERROR] {e}\")\n\n    # ---- \"Baked in\" Daemons ----\n    def hotword_daemon(self):\n        print(\"[JEMAI] Hotword daemon loaded. (stub)\")\n        while self.running:\n            time.sleep(10)\n\n    def voice_daemon(self):\n        print(\"[JEMAI] Voice system loaded. (stub)\")\n        while self.running:\n            time.sleep(10)\n\n    def ha_daemon(self):\n        print(\"[JEMAI] Home Assistant core loaded. (stub)\")\n        while self.running:\n            time.sleep(10)\n\n    def debugger_daemon(self):\n        while self.running:\n            time.sleep(20)\n            print(\"[DEBUG] jemai.py self-check: Everything running.\")\n\n    def anykey_builder(self):\n        while self.running:\n            time.sleep(5)\n            # Placeholder for live expansion\n\ndef ollama_chat(prompt, model=\"llama3:latest\"):\n    try:\n        result = subprocess.run(\n            [\"ollama\", \"run\", model, prompt],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            timeout=90\n        )\n        if result.returncode == 0:\n            return result.stdout.strip()\n        else:\n            return f\"[OLLAMA ERROR] {result.stderr.strip()}\"\n    except Exception as e:\n        return f\"[OLLAMA EXCEPTION] {str(e)}\"\n\ndef synapz_think(prompt):\n    return f\"[SYNAPZ] Thinking about: {prompt}\"\n\ndef read_file(path):\n    try:\n        with open(path, \"r\") as f:\n            return f.read()\n    except Exception as e:\n        return f\"[READ ERROR] {e}\"\n\ndef write_file(path, content):\n    try:\n        with open(path, \"w\") as f:\n            f.write(content)\n        return \"[WRITE] Success.\"\n    except Exception as e:\n        return f\"[WRITE ERROR] {e}\"\n\ndef run_shell(cmd):\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)\n        return result.stdout.strip() if result.returncode == 0 else result.stderr.strip()\n    except Exception as e:\n        return f\"[SHELL ERROR] {e}\"\n\ndef list_dir(path):\n    try:\n        return \"\\n\".join(os.listdir(path))\n    except Exception as e:\n        return f\"[LS ERROR] {e}\"\n\nif __name__ == \"__main__\":\n    JemaiCore().input_loop()\n