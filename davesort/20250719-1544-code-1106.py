\n# rag_lt_server.py\n# Local Synapz Flask API \u2014 RAG backend over Gemini history\n\nfrom flask import Flask, request, jsonify\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.llms import Ollama\nfrom langchain.chains import RetrievalQA\n\napp = Flask(__name__)\n\n# --- CONFIG ---\nCHROMA_DIR = \"./chroma_data\"\nEMBED_MODEL = \"BAAI/bge-small-en-v1.5\"\nOLLAMA_MODEL = \"llama3:8b\"\n\n# --- Initialize ---\nembedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\ndb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embedder)\nretriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\nllm = Ollama(model=OLLAMA_MODEL)\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=retriever,\n    return_source_documents=True\n)\n\n# --- Routes ---\n@app.route(\"/query\", methods=[\"POST\"])\ndef query():\n    data = request.get_json()\n    user_query = data.get(\"query\", \"\")\n    if not user_query:\n        return jsonify({\"error\": \"No query provided\"}), 400\n    try:\n        result = qa_chain.run(user_query)\n        return jsonify({\n            \"response\": result,\n            \"source\": \"Synapz RAG (local)\"\n        })\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500\n\n# --- Run Server ---\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=11435)\n