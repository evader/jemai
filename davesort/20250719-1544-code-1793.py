\\nimport os\\nimport subprocess\\nimport json\\nimport requests\\nimport sys\\n\\n# --- Configuration ---\\n# Base directory for all downloads\\nBASE_DOWNLOAD_DIR = \\\"/ai_offline_setup\\\"\\n\\n# Ubuntu version for .deb packages (important for CUDA and general debs)\\nUBUNTU_VERSION = \\\"22.04\\\" \\n\\n# Choose your GPU type - \\\"nvidia\\\" or \\\"amd\\\" (NVIDIA is much better supported for AI)\\nGPU_TYPE = \\\"nvidia\\\" \\n\\n# For NVIDIA, specify CUDA version. Visit https://pytorch.org/get-started/locally/ for compatible versions.\\n# Example: 11.8 or 12.1. Make sure it matches your driver compatibility.\\nCUDA_VERSION = \\\"11.8\\\" \\n# Corresponding PyTorch CUDA version string (e.g., 'cu118', 'cu121')\\nPYTORCH_CUDA_STR = f\\\"cu{CUDA_VERSION.replace('.', '')}\\\" \\n\\n# Python version for the Conda environment\\nPYTHON_VERSION = \\\"3.10\\\" \\n\\n# --- AI Components to Download ---\\nDOWNLOAD_LLM_WEBUI = True\\nDOWNLOAD_STABLE_DIFFUSION_WEBUI = True\\nDOWNLOAD_COMFYUI = True\\nDOWNLOAD_LLAMACPP = True\\nDOWNLOAD_WHISPERCPP = True\\nDOWNLOAD_BARK_MODELS = False # Bark models are very large\\n\\n# --- Hugging Face Models to Download (Adjust these lists as needed) ---\\n# LLM Models (GGUF for llama.cpp/oobabooga, HF_TRANSFORMERS for generic)\\nLLM_MODELS_HUGGINGFACE = [\\n    \\\"TheBloke/Llama-2-7B-Chat-GGUF\\\", # GGUF format for llama.cpp or text-generation-webui\\n    # \\\"meta-llama/Llama-2-7b-chat-hf\\\", # Hugging Face transformers format (much larger)\\n    # Add more LLMs here, e.g., \\\"stabilityai/StableBeluga-7B\\\"\\n]\\n\\n# Stable Diffusion Models (Hugging Face diffusers format)\\nSTABLE_DIFFUSION_MODELS_HUGGINGFACE = [\\n    \\\"stabilityai/stable-diffusion-xl-base-1.0\\\",\\n    \\\"stabilityai/stable-diffusion-xl-refiner-1.0\\\",\\n    \\\"runwayml/stable-diffusion-v1-5\\\",\\n    # Add more SD models here, e.g., \\\"stabilityai/sdxl-turbo\\\"\\n]\\n\\n# Whisper ASR Models (Hugging Face)\\nWHISPER_MODELS_HUGGINGFACE = [\\n    \\\"openai/whisper-large-v3\\\", # This is the full PyTorch model, used by HuggingFace Transformers\\n]\\n# For whisper.cpp, you'd typically download specific GGML/GGUF models separately,\\n# often from https://huggingface.co/ggerganov/whisper.cpp/tree/main.\\n# The script can't auto-download arbitrary files from a tree, so include them in `ADDITIONAL_DOWNLOADS` if needed.\\n\\n# --- Additional Downloads (e.g., CivitAI models, specific binaries) ---\\n# Provide direct download URLs and target filenames/paths within BASE_DOWNLOAD_DIR\\nADDITIONAL_DOWNLOADS = [\\n    # Example for a Stable Diffusion checkpoint from CivitAI (replace with actual model URL)\\n    # {\\\"url\\\": \\\"https://civitai.com/api/download/models/12345\\\", \\\"target_path\\\": \\\"ai_models/stable_diffusion/my_favorite_model.safetensors\\\"},\\n]\\n\\n# --- Internal Paths (Do not modify unless you know what you are doing) ---\\nDEBS_PATH = os.path.join(BASE_DOWNLOAD_DIR, \\\"ubuntu_debs\\\")\\nNVIDIA_CUDA_PATH = os.path.join(BASE_DOWNLOAD_DIR, \\\"nvidia_cuda\\\")\\nMINICONDA_PATH = os.path.join(BASE_DOWNLOAD_DIR, \\\"miniconda\\\")\\nPYTHON_PACKAGES_PATH = os.path.join(BASE_DOWNLOAD_DIR, \\\"python_packages\\\")\\nGIT_REPOS_PATH = os.path.join(BASE_DOWNLOAD_DIR, \\\"git_repos\\\")\\nAI_MODELS_PATH = os.path.join(BASE_DOWNLOAD_DIR, \\\"ai_models\\\")\\nSCRIPTS_PATH = os.path.join(BASE_DOWNLOAD_DIR, \\\"scripts\\\")\\n\\n# --- Helper Functions ---\\n\\ndef create_directories():\\n    \\\"\\\"\\\"Creates the necessary directory structure.\\\"\\\"\\\"\\n    print(f\\\"Creating download directories under: {BASE_DOWNLOAD_DIR}\\\")\\n    for path in [DEBS_PATH, NVIDIA_CUDA_PATH, MINICONDA_PATH, PYTHON_PACKAGES_PATH, \\n                 GIT_REPOS_PATH, AI_MODELS_PATH, SCRIPTS_PATH]:\\n        os.makedirs(path, exist_ok=True)\\n    \\n    # Subdirectories for Python packages and AI models\\n    os.makedirs(os.path.join(PYTHON_PACKAGES_PATH, \\\"pytorch\\\"), exist_ok=True)\\n    os.makedirs(os.path.join(PYTHON_PACKAGES_PATH, \\\"tensorflow\\\"), exist_ok=True) # Even if not using, good to have\\n    os.makedirs(os.path.join(PYTHON_PACKAGES_PATH, \\\"common_libs\\\"), exist_ok=True)\\n    os.makedirs(os.path.join(AI_MODELS_PATH, \\\"llm\\\"), exist_ok=True)\\n    os.makedirs(os.path.join(AI_MODELS_PATH, \\\"stable_diffusion\\\"), exist_ok=True)\\n    os.makedirs(os.path.join(AI_MODELS_PATH, \\\"audio\\\"), exist_ok=True)\\n    print(\\\"Directories created.\\\")\\n\\ndef run_command(command, cwd=None, check=True, shell=False, capture_output=False):\\n    \\\"\\\"\\\"Executes a shell command.\\\"\\\"\\\"\\n    print(f\\\"\\\\nExecuting: {' '.join(command) if isinstance(command, list) else command}\\\")\\n    try:\\n        if capture_output:\\n            result = subprocess.run(command, cwd=cwd, check=check, shell=shell, capture_output=True, text=True)\\n            print(result.stdout)\\n            if result.stderr:\\n                print(f\\\"Stderr: {result.stderr}\\\")\\n            return result\\n        else:\\n            subprocess.run(command, cwd=cwd, check=check, shell=shell)\\n    except subprocess.CalledProcessError as e:\\n        print(f\\\"Error executing command: {e}\\\")\\n        if capture_output:\\n            print(f\\\"Stdout: {e.stdout}\\\\nStderr: {e.stderr}\\\")\\n        sys.exit(1)\\n    except FileNotFoundError:\\n        print(f\\\"Command not found. Make sure '{command[0] if isinstance(command, list) else command.split(' ')[0]}' is in your PATH.\\\")\\n        sys.exit(1)\\n\\ndef download_file(url, target_path):\\n    \\\"\\\"\\\"Downloads a file from a URL.\\\"\\\"\\\"\\n    print(f\\\"Downloading {url} to {target_path}...\\\")\\n    try:\\n        with requests.get(url, stream=True) as r:\\n            r.raise_for_status()\\n            with open(target_path, 'wb') as f:\\n                for chunk in r.iter_content(chunk_size=8192):\\n                    f.write(chunk)\\n        print(f\\\"Downloaded {target_path}\\\")\\n    except requests.exceptions.RequestException as e:\\n        print(f\\\"Error downloading {url}: {e}\\\")\\n        sys.exit(1)\\n\\ndef download_system_debs():\\n    \\\"\\\"\\\"Downloads essential Ubuntu .deb packages.\\\"\\\"\\\"\\n    print(\\\"Downloading essential Ubuntu .deb packages...\\\")\\n    print(\\\"You might be prompted for your sudo password.\\\")\\n    \\n    # Common packages needed for build tools, python, git, etc.\\n    # Note: --no-install-recommends helps reduce the number of dependency downloads\\n    packages = [\\n        \\\"git\\\", \\\"build-essential\\\", \\\"htop\\\", \\\"screenfetch\\\", \\\"curl\\\", \\\"wget\\\", \\\"ca-certificates\\\",\\n        \\\"python3-dev\\\", \\\"python3-venv\\\", \\\"libsndfile1-dev\\\", \\\"ffmpeg\\\", \\\"libssl-dev\\\", \\\"cmake\\\",\\n        \\\"libgl1-mesa-glx\\\", \\\"libglib2.0-0\\\", # Common display/graphics libs for UIs\\n    ]\\n    \\n    # Try to download NVtop for NVIDIA users\\n    if GPU_TYPE == \\\"nvidia\\\":\\n        packages.append(\\\"nvtop\\\")\\n\\n    # This command downloads packages into /var/cache/apt/archives\\n    run_command([\\\"sudo\\\", \\\"apt\\\", \\\"update\\\"]) # Requires internet initially\\n    run_command([\\\"sudo\\\", \\\"apt\\\", \\\"install\\\", \\\"--download-only\\\", \\\"--no-install-recommends\\\"] + packages)\\n    \\n    # Copy downloaded .deb files to our designated directory\\n    run_command([\\\"sudo\\\", \\\"cp\\\", \\\"/var/cache/apt/archives/*.deb\\\", DEBS_PATH])\\n    print(\\\"Finished downloading Ubuntu .deb packages.\\\")\\n\\ndef download_nvidia_drivers_cuda_cudnn():\\n    \\\"\\\"\\\"Downloads NVIDIA CUDA Toolkit .deb local installer and cuDNN.\\\"\\\"\\\"\\n    if GPU_TYPE != \\\"nvidia\\\":\\n        print(\\\"Skipping NVIDIA downloads as GPU_TYPE is not 'nvidia'.\\\")\\n        return\\n\\n    print(\\\"Downloading NVIDIA CUDA Toolkit and cuDNN...\\\")\\n\\n    # Find the correct CUDA Toolkit local .deb installer URL\\n    # This URL might change, always check NVIDIA's official site:\\n    # https://developer.nvidia.com/cuda-downloads (Linux -> x86_64 -> Ubuntu -> UBUNTU_VERSION -> deb (local))\\n    \\n    # Example for CUDA 11.8 on Ubuntu 22.04:\\n    # https://developer.nvidia.com/downloads/cuda/11.8.0/local_installers/cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb\\n    cuda_url_base = f\\\"https://developer.nvidia.com/downloads/cuda/{CUDA_VERSION.replace('.', '')}0/local_installers/\\\"\\n    cuda_deb_filename = f\\\"cuda-repo-ubuntu{UBUNTU_VERSION.replace('.', '')}-{CUDA_VERSION.replace('.', '-')}-local_{CUDA_VERSION}-*-1_amd64.deb\\\"\\n    \\n    # Unfortunately, the exact filename with build numbers changes. Better to provide a direct, manually verified link.\\n    # For robust scripting, it's better to fetch from a known stable source or hardcode tested URLs.\\n    if CUDA_VERSION == \\\"11.8\\\" and UBUNTU_VERSION == \\\"22.04\\\":\\n        cuda_installer_url = \\\"https://developer.nvidia.com/downloads/cuda/11.8.0/local_installers/cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb\\\"\\n    elif CUDA_VERSION == \\\"12.1\\\" and UBUNTU_VERSION == \\\"22.04\\\":\\n         cuda_installer_url = \\\"https://developer.nvidia.com/downloads/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\\\"\\n    else:\\n        print(f\\\"Warning: CUDA {CUDA_VERSION} for Ubuntu {UBUNTU_VERSION} not hardcoded. Please manually download from NVIDIA and place in {NVIDIA_CUDA_PATH}.\\\")\\n        print(\\\"Skipping automatic CUDA .deb download.\\\")\\n        print(\\\"You'll need to manually ensure it includes the CUDA toolkit and drivers for offline installation.\\\")\\n        cuda_installer_url = None # Placeholder if we don't have a specific URL\\n\\n    if cuda_installer_url:\\n        download_file(cuda_installer_url, os.path.join(NVIDIA_CUDA_PATH, os.path.basename(cuda_installer_url)))\\n\\n    # Download cuDNN (requires NVIDIA Developer Program account and login)\\n    # This is often done manually because of the login wall.\\n    print(\\\"For cuDNN, you must download it manually from https://developer.nvidia.com/rdp/cudnn-download-survey\\\")\\n    print(f\\\"Place the cuDNN `tar.xz` or `tgz` file for CUDA {CUDA_VERSION} in: {NVIDIA_CUDA_PATH}\\\")\\n    print(\\\"Example filename: cudnn-linux-x86_64-8.9.7.29_cuda11-archive.tar.xz\\\")\\n    print(\\\"You will install it later by extracting and copying to /usr/local/cuda.\\\")\\n\\n    print(\\\"Finished NVIDIA CUDA/cuDNN instructions. Don't forget cuDNN manual download.\\\")\\n\\n\\ndef download_miniconda():\\n    \\\"\\\"\\\"Downloads the Miniconda installer.\\\"\\\"\\\"\\n    print(\\\"Downloading Miniconda installer...\\\")\\n    miniconda_url = \\\"https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\\\"\\n    download_file(miniconda_url, os.path.join(MINICONDA_PATH, \\\"Miniconda3-latest-Linux-x86_64.sh\\\"))\\n    print(\\\"Finished downloading Miniconda.\\\")\\n\\n\\ndef download_python_packages():\\n    \\\"\\\"\\\"Downloads all Python packages using pip download.\\\"\\\"\\\"\\n    print(\\\"Downloading Python packages...\\\")\\n\\n    common_pip_packages = [\\n        \\\"transformers\\\", \\\"accelerate\\\", \\\"safetensors\\\", \\\"datasets\\\", \\\"trl\\\", \\\"xformers\\\",\\n        \\\"onnxruntime\\\", \\\"sentencepiece\\\", \\\"tokenizers\\\", \\\"optimum\\\", \\\"jupyter\\\", \\\"matplotlib\\\",\\n        \\\"scikit-learn\\\", \\\"scipy\\\", \\\"notebook\\\", \\\"rich\\\", \\\"ipywidgets\\\", \\\"tabulate\\\",\\n        \\\"flash-attn\\\", # Often needs special build for CUDA, `--no-binary :all: --no-deps` is a good start\\n        \\\"bitsandbytes\\\", # Specific CUDA build, `--no-binary :all: --no-deps` for pre-download\\n        \\\"langchain\\\", \\\"llama-index\\\", # Common LLM frameworks\\n        \\\"opencv-python-headless\\\", \\\"scikit-image\\\", # Image processing for SD UIs\\n        \\\"invisible-watermark\\\", # For Stable Diffusion\\n    ]\\n\\n    # Create a temporary venv to run pip download\\n    temp_venv_path = os.path.join(BASE_DOWNLOAD_DIR, \\\"temp_venv_for_pip_download\\\")\\n    run_command([sys.executable, \\\"-m\\\", \\\"venv\\\", temp_venv_path])\\n    pip_executable = os.path.join(temp_venv_path, \\\"bin\\\", \\\"pip\\\")\\n\\n    # Install pip and setuptools in the temporary venv to ensure they are up-to-date\\n    run_command([pip_executable, \\\"install\\\", \\\"--upgrade\\\", \\\"pip\\\", \\\"setuptools\\\", \\\"wheel\\\"])\\n    \\n    # Install huggingface_hub so we can use huggingface-cli later\\n    run_command([pip_executable, \\\"install\\\", \\\"huggingface_hub\\\"])\\n\\n    # PyTorch\\n    print(\\\"\\\\n--- Downloading PyTorch and related packages ---\\\")\\n    torch_packages = [\\n        f\\\"torch==2.1.2+{PYTORCH_CUDA_STR}\\\", # Adjust version if needed\\n        f\\\"torchvision==0.16.2+{PYTORCH_CUDA_STR}\\\",\\n        f\\\"torchaudio==2.1.2+{PYTORCH_CUDA_STR}\\\",\\n    ]\\n    pytorch_extra_index_url = f\\\"https://download.pytorch.org/whl/{PYTORCH_CUDA_STR}\\\"\\n    run_command([pip_executable, \\\"download\\\", \\\"-d\\\", os.path.join(PYTHON_PACKAGES_PATH, \\\"pytorch\\\"), \\n                 \\\"--extra-index-url\\\", pytorch_extra_index_url] + torch_packages)\\n\\n    # TensorFlow (if desired, make sure CUDA version compatibility)\\n    print(\\\"\\\\n--- Downloading TensorFlow (optional) ---\\\")\\n    # TF has specific CUDA/cuDNN requirements. Check TF docs for your desired version.\\n    # Example for TF 2.15 with CUDA 11.8:\\n    tf_packages = [\\\"tensorflow[and-cuda]==2.15.0\\\"] # This pulls many deps for CUDA, can be huge\\n    try:\\n        run_command([pip_executable, \\\"download\\\", \\\"-d\\\", os.path.join(PYTHON_PACKAGES_PATH, \\\"tensorflow\\\")] + tf_packages)\\n    except Exception:\\n        print(\\\"Skipping TensorFlow download due to potential issues or if not explicitly needed.\\\")\\n        print(\\\"If you need it, verify exact versions and try again.\\\")\\n        \\n\\n    # Common AI Libraries for LLMs and Stable Diffusion\\n    print(\\\"\\\\n--- Downloading Common AI Libraries ---\\\")\\n    # For bitsandbytes and flash-attn, sometimes direct download of wheel or source compile is needed.\\n    # `--no-binary :all: --no-deps` means download source or universal wheel, but not pre-compiled specific wheels.\\n    # This ensures maximum compatibility for offline installation as a fallback.\\n    # You might need to manually get `bitsandbytes` wheels from their GitHub releases.\\n    \\n    # Try one go. If it fails due to specific packages, break it down.\\n    # Example for oobabooga dependencies:\\n    oobabooga_deps = [\\n        \\\"Jinja2\\\", \\\"gradio\\\", \\\"markdown-it-py\\\", \\\"Pygments\\\", \\\"linkify-it-py\\\", \\\"mdurl\\\",\\n        \\\"requests\\\", \\\"sentencepiece\\\", \\\"tk\\\", \\\"tqdm\\\", \\\"uvicorn\\\", \\\"websockets\\\", \\\"starlette\\\",\\n        \\\"fastapi\\\", \\\"pydantic\\\", \\\"typing-extensions\\\", \\\"sse_starlette\\\", \\\"httpx\\\", \\\"aiofiles\\\",\\n        \\\"scipy\\\", \\\"numexpr\\\", \\\"ninja\\\"\\n    ]\\n    \\n    # Example for Automatic1111/ComfyUI dependencies:\\n    sd_deps = [\\n        \\\"Pillow\\\", \\\"numpy\\\", \\\"scipy\\\", \\\"tqdm\\\", \\\"requests\\\", \\\"huggingface_hub\\\", \\\"diffusers\\\",\\n        \\\"accelerate\\\", \\\"safetensors\\\", \\\"omegaconf\\\", \\\"einops\\\", \\\"jsonmerge\\\", \\\"cleanfid\\\",\\n        \\\"resize-right\\\", \\\"torchmetrics\\\", \\\"clip\\\", \\\"k-diffusion\\\", \\\"open_clip_torch\\\",\\n        \\\"exifread\\\", \\\"piexif\\\", \\\"send2trash\\\", \\\"pyyaml\\\", \\\"mediapipe\\\", \\\"facexlib\\\", \\\"gfpgan\\\", \\\"realesrgan\\\",\\n        \\\"basicsr\\\", \\\"gradio==3.32.0\\\", # A1111 often pins gradio. Check their webui-user.bat/sh\\n    ]\\n    \\n    # Combine and deduplicate\\n    all_common_packages = list(set(common_pip_packages + oobabooga_deps + sd_deps))\\n\\n    run_command([pip_executable, \\\"download\\\", \\\"-d\\\", os.path.join(PYTHON_PACKAGES_PATH, \\\"common_libs\\\")] + all_common_packages)\\n    \\n    # Cleanup temporary venv\\n    print(\\\"Cleaning up temporary pip download environment...\\\")\\n    run_command([\\\"rm\\\", \\\"-rf\\\", temp_venv_path])\\n\\n    print(\\\"Finished downloading Python packages.\\\")\\n\\n\\ndef clone_git_repos():\\n    \\\"\\\"\\\"Clones necessary Git repositories.\\\"\\\"\\\"\\n    print(\\\"Cloning Git repositories...\\\")\\n    \\n    if DOWNLOAD_LLM_WEBUI:\\n        run_command([\\\"git\\\", \\\"clone\\\", \\\"https://github.com/oobabooga/text-generation-webui.git\\\", \\n                     os.path.join(GIT_REPOS_PATH, \\\"text-generation-webui\\\")])\\n    if DOWNLOAD_STABLE_DIFFUSION_WEBUI:\\n        run_command([\\\"git\\\", \\\"clone\\\", \\\"https://github.com/AUTOMATIC1111/stable-diffusion-webui.git\\\", \\n                     os.path.join(GIT_REPOS_PATH, \\\"stable-diffusion-webui\\\")])\\n    if DOWNLOAD_COMFYUI:\\n        run_command([\\\"git\\\", \\\"clone\\\", \\\"https://github.com/comfyanonymous/ComfyUI.git\\\", \\n                     os.path.join(GIT_REPOS_PATH, \\\"comfyui\\\")])\\n    if DOWNLOAD_LLAMACPP:\\n        run_command([\\\"git\\\", \\\"clone\\\", \\\"https://github.com/ggerganov/llama.cpp.git\\\", \\n                     os.path.join(GIT_REPOS_PATH, \\\"llama.cpp\\\")])\\n    if DOWNLOAD_WHISPERCPP:\\n        run_command([\\\"git\\\", \\\"clone\\\", \\\"https://github.com/ggerganov/whisper.cpp.git\\\", \\n                     os.path.join(GIT_REPOS_PATH, \\\"whisper.cpp\\\")])\\n    if DOWNLOAD_BARK_MODELS: # Bark is a Python library, not a standalone UI\\n        run_command([\\\"git\\\", \\\"clone\\\", \\\"https://github.com/suno-ai/bark.git\\\", \\n                     os.path.join(GIT_REPOS_PATH, \\\"bark\\\")])\\n                     \\n    print(\\\"Finished cloning Git repositories.\\\")\\n\\n\\ndef download_ai_models():\\n    \\\"\\\"\\\"Downloads AI models from Hugging Face.\\\"\\\"\\\"\\n    print(\\\"Downloading AI models from Hugging Face...\\\")\\n    \\n    # Ensure huggingface_hub is installed, it was already installed in temp_venv\\n    # As a fallback, install here globally if the script is run without venv support\\n    try:\\n        import huggingface_hub\\n    except ImportError:\\n        print(\\\"huggingface_hub not found. Installing now...\\\")\\n        run_command([\\\"pip\\\", \\\"install\\\", \\\"huggingface_hub\\\"], check=False) # Non-critical if it fails\\n\\n    from huggingface_hub import HfApi, snapshot_download\\n\\n    api = HfApi()\\n\\n    # Download LLM models\\n    for model_id in LLM_MODELS_HUGGINGFACE:\\n        target_dir = os.path.join(AI_MODELS_PATH, \\\"llm\\\", model_id.split('/')[-1].replace('.', '_'))\\n        print(f\\\"Downloading LLM model: {model_id} to {target_dir}\\\")\\n        try:\\n            snapshot_download(repo_id=model_id, local_dir=target_dir, local_dir_use_symlinks=False)\\n        except Exception as e:\\n            print(f\\\"Warning: Failed to download {model_id}: {e}\\\")\\n            print(\\\"Try manually with `huggingface-cli download <repo_id> --local-dir <path> --local-dir-use-symlinks False`\\\")\\n\\n    # Download Stable Diffusion models\\n    for model_id in STABLE_DIFFUSION_MODELS_HUGGINGFACE:\\n        target_dir = os.path.join(AI_MODELS_PATH, \\\"stable_diffusion\\\", model_id.split('/')[-1].replace('.', '_'))\\n        print(f\\\"Downloading Stable Diffusion model: {model_id} to {target_dir}\\\")\\n        try:\\n            snapshot_download(repo_id=model_id, local_dir=target_dir, local_dir_use_symlinks=False)\\n        except Exception as e:\\n            print(f\\\"Warning: Failed to download {model_id}: {e}\\\")\\n            print(\\\"Try manually with `huggingface-cli download <repo_id> --local-dir <path> --local-dir-use-symlinks False`\\\")\\n\\n    # Download Whisper models\\n    for model_id in WHISPER_MODELS_HUGGINGFACE:\\n        target_dir = os.path.join(AI_MODELS_PATH, \\\"audio\\\", model_id.split('/')[-1].replace('.', '_'))\\n        print(f\\\"Downloading Whisper model: {model_id} to {target_dir}\\\")\\n        try:\\n            snapshot_download(repo_id=model_id, local_dir=target_dir, local_dir_use_symlinks=False)\\n        except Exception as e:\\n            print(f\\\"Warning: Failed to download {model_id}: {e}\\\")\\n            print(\\\"Try manually with `huggingface-cli download <repo_id> --local-dir <path> --local-dir-use-symlinks False`\\\")\\n            \\n    # Download Bark models (if enabled)\\n    if DOWNLOAD_BARK_MODELS:\\n        print(\\\"\\\\n--- Preparing Bark model download instructions ---\\\")\\n        print(\\\"Bark models are often downloaded automatically by the `suno-ai/bark` library.\\\")\\n        print(\\\"To pre-download, you can try running their download script or let it download on first run.\\\")\\n        # Alternatively, find the model IDs on Hugging Face and add them to LLM_MODELS_HUGGINGFACE,\\n        # e.g., \\\"suno/bark-small\\\", \\\"suno/bark-coarse\\\" etc.\\n\\n    print(\\\"Finished downloading AI models.\\\")\\n\\ndef download_additional_files():\\n    \\\"\\\"\\\"Downloads any manually specified additional files.\\\"\\\"\\\"\\n    if not ADDITIONAL_DOWNLOADS:\\n        print(\\\"No additional files specified for download.\\\")\\n        return\\n\\n    print(\\\"Downloading additional files...\\\")\\n    for item in ADDITIONAL_DOWNLOADS:\\n        target_full_path = os.path.join(BASE_DOWNLOAD_DIR, item[\\\"target_path\\\"])\\n        os.makedirs(os.path.dirname(target_full_path), exist_ok=True)\\n        download_file(item[\\\"url\\\"], target_full_path)\\n    print(\\\"Finished downloading additional files.\\\")\\n\\n\\ndef generate_offline_install_script():\\n    \\\"\\\"\\\"Generates the shell script for offline installation.\\\"\\\"\\\"\\n    script_path = os.path.join(SCRIPTS_PATH, \\\"install_offline_ai.sh\\\")\\n    with open(script_path, \\\"w\\\") as f:\\n        f.write(\\\"#!/bin/bash\\\\n\\\")\\n        f.write(\\\"# This script automates the OFFline installation of the AI OS components.\\\\n\\\")\\n        f.write(\\\"# It assumes all necessary files have been downloaded to the /opt/ai_offline_setup directory.\\\\n\\\")\\n        f.write(\\\"# IMPORTANT: Execute each step manually and verify success before proceeding.\\\\n\\\")\\n        f.write(\\\"# Most steps require 'sudo'.\\\\n\\\")\\n        f.write(\\\"\\\\n\\\")\\n        f.write(\\\"OFFLINE_SETUP_DIR=\\\\\\\"/opt/ai_offline_setup\\\\\\\"\\\\n\\\")\\n        f.write(\\\"PYTHON_VERSION=\\\\\\\"{}\\\\\\\"\\\\n\\\".format(PYTHON_VERSION))\\n        f.write(\\\"PYTORCH_CUDA_STR=\\\\\\\"{}\\\\\\\"\\\\n\\\".format(PYTORCH_CUDA_STR))\\n        f.write(\\\"CUDA_VERSION=\\\\\\\"{}\\\\\\\"\\\\n\\\".format(CUDA_VERSION))\\n        f.write(\\\"\\\\n\\\")\\n        f.write(\\\"set -e # Exit immediately if a command exits with a non-zero status.\\\\n\\\")\\n        f.write(\\\"\\\\n\\\")\\n        \\n        f.write(\\\"echo \\\\\\\"-------------------------------------------------------------------\\\\\\\"\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"STEP 0: Copy the downloaded setup files to the target machine.\\\\\\\"\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"  If you haven't already, copy the entire '{0}' folder\\\\\\\"\\\\n\\\".format(BASE_DOWNLOAD_DIR))\\n        f.write(\\\"f.write(\\\\\\\"  from your download machine to '/opt/' on this offline machine.\\\\\\\"\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"  E.g.: You are on your target offline machine. Your USB/external drive is /media/youruser/MySetupDrive.\\\\\\\"\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"  sudo mkdir -p ${OFFLINE_SETUP_DIR}\\\\\\\"\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"  sudo cp -r /media/youruser/MySetupDrive{0}/* ${OFFLINE_SETUP_DIR}/\\\\\\\"\\\\n\\\".format(BASE_DOWNLOAD_DIR))\\n        f.write(\\\"echo \\\\\\\"  Press ENTER to continue (after copying files).\\\\\\\"\\\\n\\\")\\n        f.write(\\\"f.write(\\\"read dummy)\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"-------------------------------------------------------------------\\\\\\\"\\\\n\\\")\\n        f.write(\\\"\\\\n\\\")\\n\\n        f.write(\\\"echo \\\\\\\"-------------------------------------------------------------------\\\\\\\"\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"STEP 1: Install essential Ubuntu packages (offline).\\\\\\\"\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"  This may take a while. Ignore 'Could not resolve' warnings.\\\\\\\"\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"  If any package fails, try installing it individually first or check dependencies.\\\\\\\"\\\\n\\\")\\n        f.write(\\\"f.write(\\\"cd ${OFFLINE_SETUP_DIR}/ubuntu_debs || { echo 'Error: Missing ubuntu_debs directory.'; exit 1; }\\\\n\\\")\\n        f.write(\\\"f.write(\\\"sudo dpkg -i --ignore-depends=libnvidia-* *.deb || echo 'dpkg warnings/errors are common. Will try to fix later.'\\\\n\\\")\\n        f.write(\\\"f.write(\\\"echo \\\\\\\"  Attempting to fix broken installs. (This will only work if all dependencies *are* present otherwise it will fail.)\\\\\\\"\\\\n\\\")\\n        # f.write(\\\"f.write(\\\"sudo apt --fix-broken install\\\\n\\\") # This requires internet\\n        f.write(\\\"echo \\\\\\\"*** IMPORTANT: If any package installation failed due to dependencies, try installing them manually in correct order or locate missing DEBs. ***\\\\\\\"\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"  Consider a brief internet connection for 'sudo apt update && sudo apt install -f' if stuck, then go offline again.\\\\\\\"\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"  Press ENTER to continue.\\\\\\\"\\\\n\\\")\\n        f.write(\\\"f.write(\\\"read dummy)\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"-------------------------------------------------------------------\\\\\\\"\\\\n\\\")\\n        f.write(\\\"\\\\n\\\")\\n\\n        if GPU_TYPE == \\\"nvidia\\\":\\n            f.write(\\\"echo \\\\\\\"-------------------------------------------------------------------\\\\\\\"\\\\n\\\")\\n            f.write(\\\"echo \\\\\\\"STEP 2: Install NVIDIA Drivers and CUDA Toolkit (offline).\\\\\\\"\\\\n\\\")\\n            f.write(\\\"echo \\\\\\\"  This will install the CUDA local repository and then the toolkit.\\\\\\\"\\\\n\\\")\\n            f.write(\\\"f.write(\\\"cd ${OFFLINE_SETUP_DIR}/nvidia_cuda || { echo 'Error: Missing nvidia_cuda directory.'; exit 1; }\\\\n\\\")\\n            f.write(\\\"f.write(\\\"LOCAL_CUDA_DEB=$(find . -name 'cuda-repo-ubuntu*.deb' | head -n 1)\\\\n\\\")\\n            f.write(\\\"f.write(\\\"if [ -z \\\\\\\"$LOCAL_CUDA_DEB\\\\\\\" ]; then echo 'No CUDA local deb found. Skipping CUDA installation.'; else sudo dpkg -i $LOCAL_CUDA_DEB; fi\\\\n\\\")\\n            f.write(\\\"f.write(\\\"echo \\\\\\\"  Attempting to update apt. (Expect warnings about not being able to fetch remote repos)\\\\\\\"\\\\n\\\")\\n            f.write(\\\"f.write(\\\"sudo apt update || echo 'apt update failed (as expected, no internet). Proceeding.'\\\\n\\\")\\n            f.write(\\\"f.write(\\\"echo \\\\\\\"  Installing CUDA toolkit (this installs from the local deb repo).\\\\\\\"\\\\n\\\")\\n            f.write(f\\\"f.write(\\\\\\\"sudo apt install -y cuda-toolkit-{CUDA_VERSION.replace('.', '-')}\\\\\\\"\\\\n\\\") # Install specific CUDA version\\n            f.write(\\\"f.write(\\\"echo \\\\\\\"  Installing general CUDA package for convenience (may pull more if available).\\\\\\\"\\\\n\\\")\\n            f.write(\\\"f.write(\\\"sudo apt install -y cuda\\\\n\\\")\\n            f.write(\\\"f.write(\\\"echo \\\\\\\"  Installing cuDNN (manual copy). You MUST have downloaded the tarball.\\\\\\\"\\\\n\\\")\\n            f.write(\\\"f.write(\\\"CUDNN_TAR=$(find . -name 'cudnn-*-cuda*.tgz' -o -name 'cudnn-*-cuda*.tar.xz' | head -n 1)\\\\n\\\")\\n            f.write(\\\"f.write(\\\"if [ -z \\\\\\\"$CUDNN_TAR\\\\\\\" ]; then echo 'No cuDNN tarball found. Skipping cuDNN installation.'; else\\\\n\\\")\\n            f.write(\\\"f.write(\\\"  tar -xf \\\\\\\"$CUDNN_TAR\\\\\\\"\\\\n\\\")\\n            f.write(\\\"f.write(\\\"  sudo cp -P cuda/include/* /usr/local/cuda/include/\\\\n\\\")\\n            f.write(\\\"f.write(\\\"  sudo cp -P cuda/lib/lib64/* /usr/local/cuda/lib64/\\\\n\\\")\\n            f.write(\\\"f.write(\\\"  sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\\\\n\\\")\\n            f.write(\\\"fi\\\\n\\\")\\n            f.write(\\\"f.write(\\\"echo \\\\\\\"  Verifying NVIDIA installation...\\\\\\\"\\\\n\\\")\\n            f.write(\\\"f.write(\\\"nvidia-smi\\\\n\\\")\\n            f.write(\\\"f.write(\\\"nvcc --version\\\\n\\\")\\n            f.write(\\\"f.write(\\\"echo \\\\\\\"  Add CUDA to PATH and LD_LIBRARY_PATH (add to ~/.bashrc for permanent).\\\\\\\"\\\\n\\\")\\n            f.write(\\\"f.write(\\\"sudo sh -c 'echo \\\\\\\"export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\\\\\\\" >> /etc/profile.d/cuda.sh'\\\\n\\\")\\n            f.write(\\\"f.write(\\\"sudo sh -c 'echo \\\\\\\"export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\\\\\\\" >> /etc/profile.d/cuda.sh'\\\\n\\\")\\n            f.write(\\\"f.write(\\\"source /etc/profile.d/cuda.sh\\\\n\\\")\\n            f.write(\\\"f.write(\\\"echo \\\\\\\"*** IMPORTANT: YOU MAY NEED TO REBOOT YOUR SYSTEM FOR DRIVERS TO TAKE EFFECT! ***\\\\\\\"\\\\n\\\")\\n            f.write(\\\"echo \\\\\\\"  After reboot, run `nvidia-smi` to confirm drivers are working.\\\\\\\"\\\\n\\\")\\n            f.write(\\\"echo \\\\\\\"  Press ENTER to continue.\\\\\\\"\\\\n\\\")\\n            f.write(\\\"f.write(\\\"read dummy)\\\\n\\\")\\n            f.write(\\\"echo \\\\\\\"-------------------------------------------------------------------\\\\\\\"\\\\n\\\")\\n            f.write(\\\"\\\\n\\\")\\n\\n        f.write(\\\"echo \\\\\\\"-------------------------------------------------------------------\\\\\\\"\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"STEP 3: Install Miniconda (offline).\\\\\\\"\\\\n\\\")\\n        f.write(\\\"f.write(\\\"cd ${OFFLINE_SETUP_DIR}/miniconda || { echo 'Error: Missing miniconda directory.'; exit 1; }\\\\n\\\")\\n        f.write(\\\"BASE_MINICONDA_INSTALL_DIR=\\\\\\\"$HOME/miniconda3\\\\\\\"\\\\n\\\")\\n        f.write(\\\"f.write(\\\"bash Miniconda3-latest-Linux-x86_64.sh -b -p ${BASE_MINICONDA_INSTALL_DIR}\\\\n\\\")\\n        f.write(\\\"f.write(\\\"echo \\\\\\\"  Initializing Conda. You will need to restart your terminal or 'source ~/.bashrc'.\\\\\\\"\\\\n\\\")\\n        f.write(\\\"f.write(\\\"${BASE_MINICONDA_INSTALL_DIR}/bin/conda init\\\\n\\\")\\n        f.write(\\\"f.write(\\\"source ~/.bashrc # For this session\\\\n\\\")\\n        f.write(\\\"f.write(\\\"echo \\\\\\\"  Press ENTER to continue (after restarting your terminal or sourcing ~/.bashrc).\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"  You should see (base) in your prompt.\\\\\\\"\\\\n\\\")\\n        f.write(\\\"f.write(\\\"read dummy)\\\\n\\\")\\n        f.write(\\\"f.write(\\\"source ${BASE_MINICONDA_INSTALL_DIR}/etc/profile.d/conda.sh # Ensure conda is activated for script\\\\n\\\")\\n        f.write(\\\"echo \\\\\\\"-------------------------------------------------------------------\\\\\\\"\\\\n\\\")\\n\nThe file is too long and its contents have been truncated.\n", "title": "Synapzfull17072025b.json", "tether_id": null}, "status": "finished_successfully", "end_turn": null, "weight": 1.0, "metadata": {"command": "context_stuff", "is_visually_hidden_from_conversation": false, "request_id": "9606d1d37f008acd-PER", "message_type": "next", "model_slug": "gpt-4-1-mini", "default_model_slug": "gpt-4-1-mini", "parent_id": "9dcc8cc0-fb3f-4994-b24a-efb4e355fa2d", "timestamp_": "absolute"}, "recipient": "all", "channel": null}, "parent": "9dcc8cc0-fb3f-4994-b24a-efb4e355fa2d", "children": ["85804cdc-cc0a-426b-af4a-eb5dab5ac175"]}, "85804cdc-cc0a-426b-af4a-eb5dab5ac175": {"id": "85804cdc-cc0a-426b-af4a-eb5dab5ac175", "message": {"id": "85804cdc-cc0a-426b-af4a-eb5dab5ac175", "author": {"role": "tool", "name": "file_search", "metadata": {}}, "create_time": 1752724705.557042, "update_time": null, "content": {"content_type": "text", "parts": ["\nPlease remember to search the user's documents if an answer to their question is not contained in the above snippets. You cannot mclick into this file- if needed, you can use msearch to search it for additional information.\n"]}, "status": "finished_successfully", "end_turn": null, "weight": 1.0, "metadata": {"is_visually_hidden_from_conversation": false, "command": "prompt", "request_id": "9606d1d37f008acd-PER", "message_type": "next", "model_slug": "gpt-4-1-mini", "default_model_slug": "gpt-4-1-mini", "parent_id": "2c89496d-3f3d-4163-9984-729fcb7427ab", "timestamp_": "absolute"}, "recipient": "all", "channel": null}, "parent": "2c89496d-3f3d-4163-9984-729fcb7427ab", "children": ["8dacefd2-2bc3-407e-9556-92704bcdae20"]}, "8dacefd2-2bc3-407e-9556-92704bcdae20": {"id": "8dacefd2-2bc3-407e-9556-92704bcdae20", "message": {"id": "8dacefd2-2bc3-407e-9556-92704bcdae20", "author": {"role": "assistant", "name": null, "metadata": {}}, "create_time": 1752724696.376473, "update_time": null, "content": {"content_type": "text", "parts": ["Got it! You want a **complete bulletproof one-shot diagnostic and fix script** that will:\n\n- Regenerate all necessary Docker setup files and configurations,\n- Rebuild your containers,\n- Clear Docker caches and images that might cause issues,\n- Ensure everything is running correctly,\n- And embed the known necessary files directly into the script itself so you can just run it and have a fresh working environment.\n\nYou also want me to provide the **full files** as part of that script for your Docker Compose setup.\n\n---\n\n### Here's a comprehensive self-contained shell script for that:\n\n