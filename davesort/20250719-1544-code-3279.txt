\n#!/bin/bash\nset -e\n\necho \"\ud83d\udd27 Setting up file and terminal access for Open WebUI and Ollama...\"\n\n# 1. Set up the Docker Compose file\ncat <<EOL > ~/synapz_core/docker-compose.yml\nversion: '3.8'\n\nservices:\n  # JEM AI's Command & Control Backend (lt.py)\n  lt-backend:\n    build:\n      context: .\n      dockerfile: lt-backend.Dockerfile # Specify the Dockerfile name\n    container_name: jemai_lt_backend\n    ports:\n      - \"5000:5001\"\n    environment:\n      - GOOGLE_API_KEY=AIzaSyBUbyMDjaSy6a6WOtC2PfRH7g_YgFVTopU\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /etc/timezone:/etc/timezone:ro\n      - ./lt.py:/app/lt.py\n      - ./templates:/app/templates\n    restart: unless-stopped\n\n  # Ollama server for local LLM inference\n  ollama:\n    image: ollama/ollama:latest\n    container_name: jemai_ollama\n    ports:\n      - \"11434:11434\"\n    volumes:\n      - ollama_data:/root/.ollama\n      - /etc/localtime:/etc/localtime:ro\n      - /etc/timezone:/etc/timezone:ro\n      - /opt/ai_offline_setup/ai_models:/root/.ollama/models_external:ro\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n    restart: unless-stopped\n    environment:\n      - OLLAMA_LLM_MAX_MEMORY=88GB\n      - OLLAMA_FLASH_ATTN=0\n      - OLLAMA_HOST=0.0.0.0\n\n  # Open WebUI for Ollama Models\n  open-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    container_name: jemai_open_webui\n    ports:\n      - \"3000:8080\"\n    volumes:\n      - open_webui_data:/app/backend/data\n    depends_on:\n      - ollama\n    environment:\n      - OLLAMA_BASE_URL=http://jemai_ollama:11434\n    restart: unless-stopped\n\n  # Jupyter Notebooks\n  jupyter:\n    image: jupyter/tensorflow-notebook:latest\n    container_name: jemai_jupyter\n    ports:\n      - \"8888:8888\"\n    volumes:\n      - /home/jemai/jemai_notebooks:/home/jovyan/work\n      - \"/mnt/unraid/ai-workspace/datasets:/home/jovyan/data:ro\"\n    environment:\n      - JUPYTER_ENABLE_LAB=yes\n      - JUPYTER_TOKEN=twozee-ai-token\n    restart: unless-stopped\n\n  # VS Code Server\n  code-server:\n    image: codercom/code-server:latest\n    container_name: jemai_code_server\n    ports:\n      - \"8080:8080\"\n    volumes:\n      - /home/jemai/jemai_projects:/home/coder/projects\n      - \"/var/run/docker.sock:/var/run/docker.sock\"\n    environment:\n      - PASSWORD=jemai-code-password\n      - DOCKER_HOST=unix:///var/run/docker.sock\n    restart: unless-stopped\n\n  # Portainer (Docker Container Management UI)\n  portainer:\n    image: portainer/portainer-ce:latest\n    container_name: jemai_portainer\n    ports:\n      - \"9000:9000\"\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - portainer_data:/data\n    restart: unless-stopped\n\nvolumes:\n  ollama_data:\n  open_webui_data:\n  portainer_data:\nEOL\n\necho \"\u2705 Docker Compose file created with necessary services.\"\n\n# 2. Set up Flask for file handling and shell command execution in Open WebUI\ncat <<EOL > ~/synapz_core/open_webui_api.py\nfrom flask import Flask, request, jsonify\nimport subprocess\nimport os\n\napp = Flask(__name__)\n\n# File upload route\n@app.route('/upload', methods=['POST'])\ndef upload_file():\n    if 'file' not in request.files:\n        return 'No file part', 400\n    file = request.files['file']\n    if file.filename == '':\n        return 'No selected file', 400\n    file.save(os.path.join('/path/to/save/directory', file.filename))\n    return 'File uploaded successfully', 200\n\n# Command execution route\n@app.route('/run_command', methods=['POST'])\ndef run_command():\n    command = request.json.get('command')\n    result = subprocess.run(command, shell=True, capture_output=True)\n    return {'output': result.stdout.decode(), 'error': result.stderr.decode()}\n\nif __name__ == '__main__':\n    app.run(host=\"0.0.0.0\", port=5000)\nEOL\n\necho \"\u2705 Flask API setup for file upload and shell command execution.\"\n\n# 3. Restart all services with the new setup\necho \"\ud83e\uddfc Restarting JEM AI Docker stack...\"\ndocker-compose down\ndocker-compose up -d\n\n# 4. Make sure everything is running and connected\necho \"\ud83d\udd0d Testing Ollama API (port 11434)...\"\ncurl -X GET http://localhost:11434/api/version\n\necho \"\ud83d\udd0d Testing Open WebUI (port 3000)...\"\ncurl -X GET http://localhost:3000\n\necho \"\u2705 All services are now up and running!\"\n\n# 5. Provide instructions to import models\necho \"\ud83d\udca1 To import models into Ollama, just place them in the models directory, and they should show up automatically in the WebUI.\"\n\necho \"\ud83d\udd04 If models are not showing up, make sure to run the following to verify model status:\"\necho \"curl -X GET http://localhost:11434/api/tags\"\necho \"curl -X GET http://localhost:11434/api/ps\"\n\necho \"\ud83e\uddd1\u200d\ud83d\udcbb To check logs for the Open WebUI and Ollama services, use these commands:\"\necho \"docker logs jemai_open_webui\"\necho \"docker logs jemai_ollama\"\n\n# End\necho \"\ud83d\udd27 JEM AI environment is fully integrated and ready to use.\"\n