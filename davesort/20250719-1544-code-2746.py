\nfrom flask import Flask, request, jsonify\nfrom langchain.chains import RetrievalQA\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.llms import Ollama  # \u2705 Correct import for Ollama\n\nfrom langchain.prompts import PromptTemplate\nimport os, logging\n\nfrom shell_agent import run_shell_command, get_system_summary, check_process, list_top_cpu_processes\nfrom synapz_agents import docker_agent\n\napp = Flask(__name__)\nlogging.basicConfig(level=logging.INFO)\n\nCHROMA_PATH = \"./rag/chroma_data\"\nGENESIS_PATH = \"./persona/synapz_genesis.txt\"\n\n# \u2705 Proper instantiation with base_url override\nllm = Ollama(model=\"llama3\", base_url=\"http://localhost:11438\")\nembedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\ndb = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\nretriever = db.as_retriever()\n\n# Persona loading\nif os.path.exists(GENESIS_PATH):\n    with open(GENESIS_PATH, \"r\") as f:\n        SYNAPZ_PERSONA = f.read()\nelse:\n    SYNAPZ_PERSONA = \"You are Synapz Core, the autonomous AI daemon within JEM AI.\"\n\nprompt_template = PromptTemplate.from_template(\"\"\"\n{persona}\n\nContext:\n{context}\n\nUser Question:\n{question}\n\"\"\")\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    retriever=retriever,\n    chain_type=\"stuff\",\n    chain_type_kwargs={\"prompt\": prompt_template.partial(persona=SYNAPZ_PERSONA)}\n)\n\n@app.route('/query', methods=['POST'])\ndef query():\n    data = request.get_json()\n    question = data.get(\"query\", \"\")\n    if not question:\n        return jsonify({\"error\": \"No query provided\"}), 400\n    logging.info(f\"Received query: {question}\")\n    try:\n        result = qa_chain.run(question)\n        return jsonify({\"response\": result})\n    except Exception as e:\n        logging.exception(\"Error handling query\")\n        return jsonify({\"error\": str(e)}), 500\n\n@app.route(\"/shell\", methods=[\"POST\"])\ndef shell():\n    data = request.get_json()\n    cmd = data.get(\"command\", \"\")\n    result = run_shell_command(cmd)\n    return jsonify({\"output\": result})\n\n@app.route(\"/sysinfo\", methods=[\"GET\"])\ndef sysinfo():\n    return jsonify(get_system_summary())\n\n@app.route(\"/topcpu\", methods=[\"GET\"])\ndef topcpu():\n    return jsonify(list_top_cpu_processes())\n\n@app.route(\"/check_process\", methods=[\"POST\"])\ndef check_proc():\n    data = request.get_json()\n    pname = data.get(\"name\", \"\")\n    return jsonify({\"result\": check_process(pname)})\n\n@app.route(\"/writefile\", methods=[\"POST\"])\ndef write_file():\n    data = request.get_json()\n    path = data.get(\"path\")\n    content = data.get(\"content\")\n    if not path or not content:\n        return jsonify({\"error\": \"Missing path or content\"}), 400\n    try:\n        with open(path, \"w\") as f:\n            f.write(content)\n        return jsonify({\"status\": \"ok\"})\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500\n\n@app.route(\"/status\", methods=[\"GET\"])\ndef status():\n    return jsonify({\"status\": \"Synapz Core is active\", \"path\": os.getcwd()})\n\n@app.route(\"/agent/docker\", methods=[\"POST\"])\ndef docker_api():\n    data = request.get_json()\n    command = data.get(\"command\")\n    target = data.get(\"target\")\n    if command == \"list\":\n        return jsonify(docker_agent.list_containers())\n    elif command == \"status\":\n        return jsonify(docker_agent.get_status(target))\n    elif command == \"restart\":\n        return jsonify({\"result\": docker_agent.restart_container(target)})\n    elif command == \"logs\":\n        return jsonify({\"logs\": docker_agent.get_logs(target)})\n    else:\n        return jsonify({\"error\": \"Invalid command\"}), 400\n\nif __name__ == '__main__':\n    logging.info(\"\ud83d\udd0c Synapz Core Flask server starting...\")\n    app.run(host=\"0.0.0.0\", port=11436)\n