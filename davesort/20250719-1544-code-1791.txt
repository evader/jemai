\\n    cd /opt/ai_offline_setup/git_repos/llama.cpp\\n    make LLAMA_CUBLAS=1 # Compile with CUDA support\\n    # Models: Place your GGUF models in `llama.cpp/models/`\\n    # cp /opt/ai_offline_setup/ai_models/llm/Llama-2-7B-Chat-GGUF/* /opt/ai_offline_setup/git_repos/llama.cpp/models/\\n    ./main -m ./models/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf -p \\\"What is the capital of France?\\\" -n 128\\n