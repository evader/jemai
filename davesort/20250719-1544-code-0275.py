\n#!/usr/bin/env python3\n\nimport requests\nimport readline\nimport subprocess\nimport sys\nimport os\nimport time\n\nOLLAMA_HOST = \"http://localhost:11434\"\n\ndef check_ollama():\n    try:\n        r = requests.get(OLLAMA_HOST)\n        if r.status_code == 200:\n            return True\n    except Exception:\n        pass\n    print(\"Ollama is not running! Trying to start Ollama service...\")\n    if os.geteuid() == 0:\n        subprocess.run([\"systemctl\", \"start\", \"ollama\"])\n    else:\n        try:\n            subprocess.run([\"sudo\", \"systemctl\", \"start\", \"ollama\"])\n        except Exception:\n            print(\"Couldn't start Ollama. Please start it manually and rerun.\")\n            sys.exit(1)\n    # Give Ollama a second to boot\n    time.sleep(3)\n    try:\n        requests.get(OLLAMA_HOST)\n        return True\n    except Exception:\n        print(\"Still can't reach Ollama on http://localhost:11434.\")\n        sys.exit(1)\n\ndef get_models():\n    try:\n        resp = requests.get(f\"{OLLAMA_HOST}/api/tags\")\n        models = [m[\"name\"] for m in resp.json().get(\"models\",[])]\n        return models\n    except Exception:\n        return []\n\ndef pick_model(models):\n    if not models:\n        print(\"No models found in Ollama! Import a model first (e.g. `ollama pull llama3`).\")\n        sys.exit(1)\n    print(\"\\nAvailable models:\")\n    for i, m in enumerate(models):\n        print(f\"  [{i+1}] {m}\")\n    choice = input(f\"Pick model [1-{len(models)}] (Enter for default={models[0]}): \").strip()\n    if not choice:\n        return models[0]\n    try:\n        idx = int(choice)-1\n        if 0 <= idx < len(models):\n            return models[idx]\n    except Exception:\n        pass\n    print(\"Invalid choice.\")\n    return models[0]\n\ndef main():\n    print(\"=== Ollama CLI Chat ===\")\n    check_ollama()\n    models = get_models()\n    model = pick_model(models)\n    print(f\"\\nChatting with model: {model}\")\n    print(\"Type Ctrl+C or Ctrl+D to exit.\\n\")\n    while True:\n        try:\n            prompt = input(\"You: \")\n            if not prompt.strip():\n                continue\n            data = {\"model\": model, \"prompt\": prompt, \"stream\": False}\n            r = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=data)\n            r.raise_for_status()\n            print(\"AI:\", r.json()[\"response\"])\n        except (KeyboardInterrupt, EOFError):\n            print(\"\\nExiting.\")\n            break\n        except Exception as e:\n            print(f\"[ERROR] {e}\")\n\nif __name__ == \"__main__\":\n    main()\n